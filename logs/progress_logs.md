# autolang Progress Logs

By Lyons / fawnium

These are my personal progress logs that I am writing during the development of autolang. They are more stream-of-conscioussness than a formal documentation of the project. They are mainly intended to give an informal insight into my development process for the interested reader, and also to help me organise my thoughts so I can develop more efficiently. I wrote them in conjunction
with the code, and didn't re-draft them after the initial writing, so don't expect them to be good!

## 30/08/2025 - Log #0

I decided it would be a good idea to create a log of how the development of autolang is going, so that it is not just documented implicitly in the code. I have already written quite a lot from when I started in May/June until now, but it will be hard to remember all of this in detail, so I will simply summarise:

I first implemented minimal classes to simulate DFAs, NFAs, PDAs, and TMs. The only major function of each of these thus far is to take an input string, simulate the respective machine processing it, and returning acceptance value. The TM also returns the tape at time of halting as well as a boolean. I then realised that the specific formatting for the transition functions of each machine is quite nuanced and complicated, and varies in subtle ways for different machine types. So I refactored the transition error-checking into separate classes which only validate the data structures and don't do any simulation. The separation here is still not as clean as I would like - for instance the lists of states and alphabet must still be passed separately to the transition function and must be passed to both the transition object and the automata object itself. This feels quite redundant, and I think a better solution would be to automatically extract the states and alphabet from the transition function, but this would involve more thought about handling errors and edge cases. As it does not affect the code's function, it is not a major priority, but this will likely need another refactor later. I expect much of the existing code will need to be drastically changed or removed entirely eventually. For example, the existing machines can process input words, but do not return any logs or demonstrate step-by-step computation. This will need to change to allow nice visualisations or illustrate specific computation steps for education. The potential solutions I can think of are either: keeping the existing classes but embedding extra lines and function calls to the frontend; or building entirely new classes using what I've learned, and having two copies of each machine in the source code - one for purely returning acceptance, and one for more detailed user explanation which is more complex and includes the frontend integration. All of this will probably be done later down the line. With the basic machines already implemented, my current next step is adding regex/language features which work in tandem with the automata. I am currently working on the regex-NFA-DFA pipeline, which is still in the early stages. I have written a parser for simply validating the syntax of an input regex. Now I need to create another parser to iteratively construct the corresponding NFA itself. I considered Thompson's Construction which appears to be the canonical way to do this conversion. What I would like to instead do is implement a construction that is closer in spirit to the theoretical method/proof which appears in notes/textbooks, that is more intended for illustration and to be done with pen and paper. I believe this is spiritually the same as Thompson's construction under the hood, but I would like to do something closer to what my personal thought process was when learning the theory, and arrive on something that is not necessarily the most performant or canonical, but is reflective of how I came to understand the construction myself, and will hopefully to easier for students to directly connect to the theory they are learning. Perhaps I will 'accidentally' reproduce Thompson anyway...

After regex-NFA, the next natural step is NFA-DFA using the standard powerset construction, and perhaps with further state minimisation. I expect this will not be as challenging, since it more directly follows the theory, and should not involve as much parsing/lexical analysis. 

Below I list the remaining long-term goals of the project that I have thus far been thinking about. These may change, but I need to make them concrete by writing them here so I have a roadmap in mind during development:

- Finish regex-NFA-DFA
- Do similar pipeline for CFG-PDA
- Add more miscellaneous grammar features, such as converting to Chomsky Normal Form and other macros/preprocessing algorithms
- Add more (sub)classes of automata, such as multi-tape TMs and similar variations. This will likely require polymorphism, and/or further serious considerations about the overall project structure
- Implement visuals of the machines. I already experimented with drawing automata using `networkx`, but it grew very complex with edge-cases and creating nice, readable layouts. Handling arbitary-length edge labels, overlapping edges, nondeterministic- and epsilon- transitions, and general graph-readability are all hard to formalise in 
code, and will take more time to do well than I have right now. I abandoned the visualisation for the time being, as I wanted to focus on getting the backend working first.
- Further develop the frontend, such as having an interactive display of the automata, which allows adding/removing states and transitions, and a live playback of computation, which will for example highlight the current state(s) with different colours, and perhaps include some animations later. I know there are libraries to do this natively in python, but in the past I found them to be quite janky and 'ugly'. I suspect the best strategy is to write the frontend in JavaScript, but to do that I first 
need to learn JavaScript.
- Go back and re-implement several features using the above instead of native python. For example, the parser(s) for handling regex is just done with a separate python class now, but I would like to do this with autolang's own PDA feature which already exists. While regular expressions describe regular languages, the language of regular expressions *themselves* is context-free. Therefore since this is exactly what PDAs are supposed to be for, I think keeping the whole project as self-contained as possible, and recursively using some parts of the code to help other parts, would be a nice 'meta-level' flare, and really highlight how powerful these state machines are.
- Ultimate Goal: write a 'direct-to-TM' quasi-compiler. This will take either a subset of real python or a minimal custom DSL, convert it to a TM transition table in numerous steps, and then execute the code directly on a virtual Turing Machine. This is of course very ambitious, and I don't know how far I will get with this. I will need 
to sequentally design various intermediate steps to parse and process the code, build ASTs, and I aim to use autolang's DFAs/PDAs to do this as much as possible. I will then need to design a reasonable computer architecture that logically partitions the TM tape into input, memory, etc, and somehow create a library of macros that converts quasi-assembly instructions into entries in the TM transition table. I imagine this will be easier to do on a multi-tape machine, which is then converted to a single-tape 
as the final compilation step.

## 30/08/2025 - Log #1

Today the goal is to make progres on regex-NFA. I will first finish writing some unit tests for the regex syntax validation. Hopefully after that I will finally be ready to do the construction itself. I need to make a `GNFA` class, which allows edges to be labelled by arbitrary strings instead of just letters in the alphabet or epsilon. The algorithm will work by doing the following:

- start with a GNFA with only a start and accept state, and one edge labelled by the whole regex.
- iterate through the edges, find the first one with a non-trivial/non-atomic label, and parse the label.
- when parsing edge label, find the first top-level operation, and modify the GNFA according to the respective rule from the theoretical proof - e.g. for union `+`, delete the initial edge and make two new edges labelled by the child-regexes.
- stop when all labels are atomic, meaning hopefully we should have a valid (non-G)NFA!
After this, I will write more tests to ensure the resulting NFA is correct. These shouldn't be very hard - it should simply be a matter of coming up with some example regexes, and checking the language of the NFA matches the actual language of the input regex, the latter being generated using existing functions from `utils.py`

Finished tests, adding concat, and the GNFA class which handles construction logic. Still need to implement the parser that decides what operation to eliminate.

## 31/08/2025 - Log #2
Today the aim is to implement the parser which performs an elementary elimination on a regex string. After that, the entire regex-NFA construction should be complete. The parser needs to decide which of (+, ., *) is the highest-precedence operator in the regex, ensuring it remembers how deep it currently is in terms of brackets. It must only eliminate a top-level operation, i.e. outside any grouped subexpressions. After that, it should return a tuple consisting of which operator is being eliminated (`None` if regex is primitive), as well as the resulting smaller regex(es) to be used as labels for the new GNFA edges. For both union + and concat ., two regexes will be returned (the operands on each side of the operator), whereas for star * only one is returned, since * only acts on a single atom. I imagine this will be quite challenging to do correctly.

If there is time, I will also write some unit tests for the GNFA class, separate to the tests for the entire regex conversion. I already did some tests for adding explicit concat yesterday, but it is likely many more will need to be added accross the existing code. For the time being, as long as the languages match, I think it is safe to assume the entire regex-NFA conversion works correctly. We will see once the code is actually implemented...

## 03/09/2025 - Log #3

I have made some progress in the last few days, but it has been slower than expected, due to other time commitments and complications in the implementation. The GNFA class seems to working fine now, and the small amount of tests all ran OK. Today the regex elimination parser will be completed. I already wrote a method to strip outer brackets. 
These arise in a regex substring if that substring was originally grouped in the parent expression, but after elimination steps has now been isolated. None of the actual operator elimination methods remove brackets from a string, so this needs to be handled in preprocessing before the core functionality of the parser executes. Otherwise, a regex such as '(a+b)' would be decomposed as '(a' and 'b)', which clearly shouldn't ever happen.

Today I will write the actual elimination method(s) of the parser. This should not actually be too difficult - it doesn't require recursive descent or anything complicated, because all regex strings are assumed to already be syntactically valid at this stage of the pipeline. The only important nuance is keeping track of bracket depth, so that for example, '(a+b)+c' is correctly decomposed as '(a+b)' and 'c', and not as '(a' and 'b)+c'. The other important thing I realised after some thinking and testing, is that it is crucial to always check for union first, then concat, and then star, and not simply return at the first top-level operator of any kind. If this was not done, then for example 'a.b+c' would be incorrectly decomposed as 'a' and 'b+c', instead of the correct 'a.b' and 'c'. This priority of operators is the correct way to capture the Thompson's Construction idea, and accurately reflects the hierarchy of the regex grammar.

This project has really got me thinking about BODMAS/PEMDAS in far more detail than I did when I was first learning algebra and arithmetic back in the day. Things like which operators bind to which terms in an expression, are quite intuitive to a human who can visually scan an entire word or string all at once, but are surprisingly difficult to rigorously formalise in code. I suppose that is precisely why finite state automata (and hence this project) were invented in the first place.
Update: I finished the regex elimination parser, and wrote a wrapper that handles the entire regex-to-nfa pipeline. After a few rudimentary unit tests...it works! I never thought that seeing a simple 'OK' printed to my screen could bring so much joy. I will try to keep these logs professional, but I honestly did not believe I would be able to pull this construction off with this few complications or setbacks. The next step is is to make sure it really works by writing some more robust tests, and then moving on to the DFA construction. I think after that would be a good point to take a step back and think about cleaning up the existing codebase, before moving on to CFGs.

## 07/09/2025 - Log #4

Today the task is to implement the NFA-to-DFA subset construction. I also need to do some more testing of regex-to-NFA, but I will probably do this afterwards because I have been finding all these tests quite tiring and monotonous (but they are important). I have been thinking about how to translate the construction in the proof into code while 
at work, and have already been taking interest in some of the arising subtleties that I will no doubt have to resolve today. For example, in the theory a DFA is just a special case of an NFA, but in my code, they both exist as effectively completely separate sections of code. In the DFA transition function, the outputted value is just a single state string (unlike a list of strings for the NFA). I could similarly have defined it to be a list containing only a single state, which would have been more 
consistent with the nondeterministic classes. I chose not to do so because I figured using a list to contain the state when there is by definition only one, will add additional indexing overhead while extracting the state, hence slightly reducing performance during runtime. I imagine these performance considerations are trivial, but it does make me really think about how to make decisions for or against performance versus clean/consistent code. That got me thinking about polymorphism again - certainly 
according to the theory, it would make sense to refactor the DFA and NFA to be instances of a base 'Automaton' class, and then only implement what strictly differs from the general case. I think this may have some educational benefit, since the code could be more exprssive and more directly demonstrate the precise differences between different automaton models. However, I also have a natural or intuitive aversion to polymorphism, likely in part because I have not really tried it yet, but also because I think it can be easy to over-engineer a complex inheritance hierarchy just for the sake of being 'clever', regardless of whether it meaningfully improves the encapsulation of the code. All the energy I must spend to use by category-theoretic brain to really think about the best way to partition automata functions using inheritance, is energy I am not spending actually writing the code and making it work. And additionally, I will likely not even be able to intuit the best inheritance structure until all of the actual code exists in front of me first. I am of course not opposed to this kind of thinking - pure math is nothing if not categorising classes of complex structures in well-defined and 'pretty' ways - but this is a different context to 'taming' opaque and complex algebras, this is about getting a code that does what is supposed to do however it can, and then thinking about restructuring and making it pretty later. I feel I am trying to think about everything at once and not making material progress anywhere. All this leads me to just stop thinking about polymorphism, and try to focus on each task in the roadmap as it comes. There will very likely be a more appropriate time to think about inheritance later down the line, and as I said in a previous log, finishing the subset construction is a natural stopping point to think about the overall architecture again. Therefore 
the most crucial thing in any case is doing the subset construction today!

Regarding implementing the subset construction, I will have to think carefully about how to build the DFA step by step. The intellectually easiest way would be to pre-populate with all possible subsets, then add the transitions, then delete the states that are not used. But this would be extremely memory-hungry for any NFA with a moderate number of states, since the growth is exponential. Therefore a better idea is some kind of 'lazy construction' where subsets are only added when strictly mandated by a transition. This means I will need to try and focus on the individual methods while having some idea of how they work together, before actually seeing how they work together, e.g. correctly generating the epsilon closure of a transition before even having an object for it to operate on. This is the eternal paradox of needing to make the parts before having the whole, while simultaneously needing to understand the whole to make the parts, and it was this kind of paradox that caused me problems with the recursive descent parser and other areas before. But this is something that needs to be done, and it is very rewarding when it all eventually works together all at once. I believe Feynman had some quote about nobody really understanding quantum mechanics, which I think also applies to recursive ideas in programming. You don't really have to understand it (if anyone even 
knows what true understanding is), so long as you can leverage it to achieve your goal and have a good working knowledge of the tools at your disposal. That being said, gaining more understanding is always better, and I feel that I have been gaining a lot of deeper understanding since starting this project. I hope that feeling is balanced more towards reality than delusion, but if it is more delusion, it has at least got me this far, and the code seems to be doing what it's supposed to. 

Update: lazy subset construction now seems like not only a better idea, but completely essential for this code to have any general-use viability. If the input NFA has 30 states, then the upper bound for the number of DFA states is already over a billion. A billion states is manageable on most hardware, but it will only get much worse from there, and certainly 30 input states is a reasonably small number that could easily get passed during runtime. Therefore lazy construction is the only way. But then my only concern is that I may have over-engineered the `TransitionDFA` error checks. It must have a transition for every possible state, but I'm not yet sure whether this will flag subset-states that technically should be members, but are never actually reachable by an input word. With lazy construction, it *should* be allowed to states that are not reachable. I think it is more likely than not to be ok as-is, because the finished DFA doesn't really 'care' about the specific NFA states from which it was built, so I will proceed with caution but not refactor the Transition structs yet. The long-term solution I had planned for this is to extract the states and alphabet lists from the transition dict directly, instead of passing them independently - this would also simplify a lot of the error handling and yield better encapsulation. But that would require a huge refactor because virtually all relevant function signatures in the project would need to be modified. As I expected, my past bad decisions are catching up with me despite my best efforts. But I said I would finish the subet construction before any refactor, so I have to stick with that. If I can't stick with a bad decision that was made in good faith, I will never get anything done. But I am definitely going to hold off on anything to do with CFGs or further features until I'm a lot more happy with the existing code's design - the first version will certainly go up on GitHub before anything else major is added.

Update #2: The NFA-DFA constructor class is now finished (I think), but I haven't ran or tested it. I followed Sipser and a pen-and-paper construction found at `https://www.youtube.com/watch?v=jMxuL4Xzi_A` to try and ensure the construction was followed correctly, but there was still a lot of re-ordering steps to make it logical and reasonably performant inside python. It may not work properly right now. However, I think now is a good time to do a big refactor as the DFA construction highlighted some formatting/data encoding inconsistencies that need to be resolved sooner than expected to avoid more technical debt. This means I will likely have to rewrite `ConstructDFA` to align with the rest of the project as it is being updated, but otherwise I would have to also rewrite the tests, so it seems pointless to do the tests now when most of the code will be torn down anyway. Either way I will find out if it works at some point, but this way I am reducing the amount of rewrites. At least now the code 
is written and I'm confident it 'mostly' works - if it doesn't this will be revealed in testing. Now I need to finish the tests for regex-NFA since they were already started, and go all the way back and rewrite 'backend/machines'

## 10/09/2025 - Log #5

I have already begun rewriting the transition wrapper classes, and modifying the automata objects accordingly. It is tricky to ensure that everything will still be passed to the automata objects as it was before, given that the initialisation arguments are being modified. The existing unit tests will likely also have to be rewritten for the same reason, but I think it is a change that is worth doing. After the rewrite, the lists of states and alphabet letters will no longer be passed as separate arguments. 
Instead, they will be directly extracted from the transition table. In my view this is a much better design choice, because there is no possible case where for example an automaton should have an internal state that is not listed in the transition function, since such a state could never actually be reached during computation. The same is true for alphabets - if a letter cannot be read by the transition function, it might as well not be in the alphabet. With this change, less information is required as input, which yields a cleaner code architecture with better encapsulation.
After this refactor, I need to go back and check the regex-DFA construction is still working. Then the only thing left is to ensure all the tests are reasonably robust before deploying version 1.0. The current plan for v1.0's features is the following: working DFAs, NFAs, PDAs, and TMs; a working regex-DFA pipeline; some examples of how to use each feature and a README; and finally the appropriate test files. That is everything for the first version. v1.1 (or whatever it'll be called) will then include some minimal visualisation - namely printing transition tables and drawing static transition diagrams. The CFG stuff will come after that. I feel I am now at the stage that mandates more detailed thinking about the versioning and overall roadmap, but I still need to focus on just getting the first version out in public. That will be the first major breather where a step back can be taken to consider the later steps.

Update: I have uncovered a subtle challenge with the new input scheme when it comes to TransitionTM. It will not be possibly to independently extract the input and tape alphabets, since the transition function does not have an intrinsic way to distinguish between letters that are and are not allowed as input. This is because unlike the PDA case, where the input and stack letters do not overlap within the data structure, the TM transition effectively works with the input and tape all pooled together during 
compute. Hence it seems the power of generality which Turing machines have, is also the reason they are difficult to implement consistently with the rest of the project. I cannot see a rigorous way of separating the input and tape alphabets without the user explicitly passing them in their entirety, which is precisely what the refactor was meant to avoid. In practice this may not be a huge hurdle - the most crucial thing is that the reserved blank letter '_' is forbidden in the input alphabet, but other than that I think in most real cases the input and tape alphabets will mostly* agree. This is a difficult problem though. The easiest solution for now is to simply always assume both alphabets agree excluding the blank letter, and handle the separation in a later version. This of course means the TM implementation does not have full generality and does not completely align correctly with the theory. But the TMs are not a main feature of v1.0, and are mainly there for demonstration and to signal the later ambitions of the project. I will have to remember to make a note of this in the README.

Update 2: It is also harder than expected to handle the accept and reject states of the TM. These should be included in the list of states according to the theoretical model, but are allowed to behave slightly differently, since they don't allow outgoing transitions. I'm also conflicted about whether to include default values for the start and accept state - if I do, these names may conflict with user-intended names in unexpected ways, but if I don't then the user has more responsibility to correctly identify the halting states, and that requires more complex input handling. Either way it is a non-trivial design problem. There is also the question of whether we should actually enforce halting states being listed in the transition function in the first place - if there is no way to enter a halting state for any input, then is this a user mistake, or did they intend to have a machine that can't halt by design? This is a lot to think about and is quite nuanced. I think these kind of questions shouldn't be thought about too much at this stage as it's slowing down progress, and the focus of v1.0 is on DFAs and NFAs anyway. Likely the best strategy will be revealed when I'm writing concrete example uses of the code later today, anyway. In any case I don't think it is logically possible to construct the TM purely from the transition function alone - there is simply not enough information encoded in it for such a complex machine.

Update 3: I am now pretty sure the refactor is finished and should keep everything working. I have not however updated the unit tests. Now I think what is important is to write some clear examples to show users how to interact with the code. Potential problems will come up if the examples fail, so if that happens, then is the right time to redesign the unit tests, in tandem with the concrete examples. I will try as much as possible to use examples directly from Sipser to have an easy and canonical reference.

## 12/09/2025 - Log #6

I have discovered a subtle and tricky bug in the nfa construction, which was exposed when testing with input '(0+1)0*'. The elimination parser needs to trim enclosing brackets each time it's passed a regex, otherwise it would not find any top-level operator for '(0+1)' for example. On the other hand, it needs to match the exact original regex when attempting to remove the corresponding edge. Therefore it needs some way to simultaneously omit arbitrarily many layers of brackets, while still remembering how many layers were there so that it can find the right edge to delete. The bug emerged because it correctly split the input into '(0+1)' and '0*', but then it turned the former into just '0+1', and then tried to find an edge labelled '0+1.0*' instead of '(0+1).0*', which was the intended deletion target. This was a nuanced problem that I did not expect, and 
it seemed to emerge from the implementation in a way that was not obviously connected to the theoretical construction. It is remarkable how many small, simple steps the human brain subconsciously omits when thinking about interpereting a string of symbols on a page. Perhaps it is because it is a skill learned so early in life and is so intuitive, which makes the study of lexical analysis all the more interesting. 

Much else has been completed including many example usage files, but I don't care to go into detail as it is already quite late. I just want to fix the edge removal bug though - I think the easiest solution is storing a copy of the edge before passing to the parser, rather than doing something clever like returning the exact number of brackets.

## 14/09/2025 - Log #7

As of today I am finished with my part-time job, so now I should have much more time to focus on autolang which is very exciting. I did a lot yesterday which I forgot to log. Most importantly I fixed the bug in the DFA subset construction by standardising subset representations as ordered tuples, which attains the best of both worlds by being deterministic and ensuring equivalent subsets with different orderings are still identified as equal, while ensuring subset representations are well-behaved and can be stored in a parent set via hashing. I previously didn't know that trying to put a set inside another set will cause problems in python, but that is both unsurprising and interesting. The example files for the main intended features are now finished. I have also decided the initial version will be called v0.1.0 - this is better than starting with v1.0, which is now intended to be the fully-fledges library with CFG features etc. Based on the examples I believe the code is working as intended, but of course the test files still have to be written. This will likely take some time to do robustly, so what is most likely is that the initial version will omit the tests which will then be added in a later commit.

Here is everything I need to do before v0.1.0 finally goes public: add a .gitignore file; properly learn how to use git and actually commit the code (I already did this for another project back in 2019 but have forgot everything); write a comprehensive README (this will likely take some time); add visualisation (I also decided this would be important to have in the initial version as it is a core feature); and finally push the code! There are likely some other steps that either I've forgotten or will emerge whilst doing the above, but now we are very close.

## 15/09/2025 - Log #8

We are in the home stretch for releasing v0.1.0, and as usual this is where things start grinding to a halt. I always find that doing the small, technical, bureaucratic details to finalise a project are harder and more time-consuming than the actual project itself, and nothing is different here. I am somehow still having issues with local imports - it seems relative imports are breaking with the `visuals/` folder. In general the code itself is not the problem, but packaging the code and enabling users to actually use it is trickier than expected. I need to make a .toml file and various other python things to get it working, and so I will likely have to do a lot of learning today. The ultimate goal is usability, so users should be able to simply do `pip install autolang` and have it just work, but I think that is still far away right now. The planned installation 
for the first release is by simply cloning the repo from github, and running the code in a more 'manual' way. I think getting this scheme working is more important than the final user case, but in any case getting the import paths correct is crucial. I am also still trying to figure out src layout so I will probably have to go and read the guidance again. This feels like a stupid set of issues to get stuck on instead of the actual automata logic, but that's software development for you I guess.

Update: I think now all the imports the __init__ file are working correctly (although this is about the 5th time I have said that). Now I am working on the transition table formatting for each automaton model. This is a different kind of problem but still interesting - the challenge now is calculating column widths correctly instead of implementing automata compute trees. The DFA table looks great, and now I am doing the NFA table which shouldn't be very different. The PDA will likely require more consideration because the header rows need to be formatted with sub-cells to list the input *and* stack letters, but that is a fun and exciting challenge for later today.

Update 2: The PDA transition table turned out to be harder than expected. Getting the formatting right is a nightmare considering that here there are effectively two different types of columns: the micro columns, which contain the actual cell values; and the macro columns, each of which includes one micro column for every letter in the stack alphabet (and epsilon). Getting this right will take more time and I don't think it is a priority. The TM transition should ironically be easier, since it is deterministic and there is only one alphabet to worry about, so I *will* get that done. It is really interesting, seeing how the theoretical differences between the abtract models translate to different problems and different degrees of complexity when implementing them in code. The DFA is the simplest model, and is also the easiest to implement in code. NFAs are slightly harder due to nondeterminism, but are still quite similar to DFAs. PDAs might actually be the hardest, because not only are they nondeterministic, but they also have two alphabets. It might be tempting to say that TMs are harder because they also technically have two alphabets, but once again they seem to be an outlier: the distinction between the alphabets only really matters when creating the machine, since after that everything just lives on a single tape; and they are deterministic which makes everything substantially easier. This is not really relevant to the progess, just an observation. 

Update 3: I also forgot another interesting observation I noticed. The main problem with the PDA table is that correctly counting the width of cells *and* including the right number of dividers between the cells is actually quite hard. This could be hacked together with some clever 'magic' lines for the DFA and NFA, but with the PDA I had to account for which dividers came from the 'macro' columns, and which for the 'micro' columns. If there was only one alphabet, this can be achieved by just looping through the letters and prepending each with one div, then appending a single div at the end for the outer border. But with two alphabets, you have to be careful with counting the divs - for example if you tried the same thing and prepended both the input *and* stack letters with a div, then you would end up with double divs at the beginning of each new macro column, which is suboptimal. There is definitely a simple way to do this but I just need more time - this is taking me back to counting intersections of subsets by iteratively adding and subtracting. Anyway, the observation is that ironically, this kind of problem seems to feel like a CFG problem. We start with a nonterminal that represents the header, then this can yield a macro nonterminal, which in turn yields the micro columns which terminate to the actual letters. I'm sure it's possible to define an actual grammar for this problem, but it wouldn't be very interesting because it can only generate one word, namely the final column header. While it would definitely be over-engineering to use a PDA to build the PDA table itself, I think it is really interesting how I'm now seeing these connections everywhere. I think this reflects the meta, self-referential theme of the 
project - we are using computers to simulate computers, so of course said computers will be relevant to solving their own problems! 

## 17/09/2025 - Log #9

I made some relaxing progress on the train yesterday and this morning. I am still working on the PDA table, but I think it is now almost done. The PDA table is more dynamic than the other tables, because it calculates an individual width for each column. It might be a good idea to change the other tables to align with this to save space on the screen wherever possible, but I will worry about that later.

## 22/09/2025 - Log #10

Unfortunately I have been suffering from a viral infection the last few days, but that has not stopped me continuing to code, albeit slowly. I am now finally completing the unittests for the project slowly but surely. I am currently rewriting the previous tests that were written before the refactor that changed all of the function signatures, and 
after that I will finish by writing tests for the rest of the files that didn't have any tests before. Unfortunately I haven't been able to run the tests during this process because the old tests (which are now broken) are still in the parent folder, and I can't be bothered moving them out. But once I have rewritten all the old tests I will be able to run them before writing the new tests. This is a little risky as it may result in a large barrage of failures all at once, but I will have to cross that bridge when it comes and fix them one by one. I would rather get all this right before the release, even if it takes more time. The progress is getting painfully slow in this finalisation state as I iron out all the subtleties. I am starting to see why 'crunch time' is so prevalent in software development.

While rewriting the tests I think I spotted another bug in the regex elimination parser, once again due to enclosing brackets. Before, a primitive regex would simply instruct the GNFA to do nothing, since no elimination needs to happen. But this would cause problems for something like '(a)', which is technically primitive, but still needs its brackets 
removed before creating the NFA. The problem was that the brackets were only removed internally in the parser, and the GNFA could not 'see' whether or not any brackets were trimmed. The fix I had for this before was to record the initial regex inside the GNFA before calling the parser. This works for something like '(a*)', but would not work for '(a)*', since this 
would produce an edge '(a)' that would fail to be caught in the next GNFA iteration. The new fix I made is that instead of the parser returning nothing for a primitive regex, it now returns the primitive regex after the brackets are removed, hence allowing it to communicate the removal to the GNFA. Passing an edge with brackets to the final NFA is a terrible idea and will lead to breaks. I believe I the NFA init error checks would catch this, but it is still not acceptable to raise an error, and worse case scenario there is no error, and instead there will be silent unintended behaviour in the NFA, which would be catastrophic. I believe now there shouldn't be more problems with bracket handling, but the communication between the parser and GNFA is quite janky, and could probably do with a rewrite. That is something for a later version.

Update: The 'bug fix' has broken all of the regex examples. I should have just left it as it was since it was working. I am going to try and revert it to how it was. I shouldn't have tried to do everything at once, I should have just focused on finishing the tests.

## 23/09/2025 - Log #11

I have now finished rewriting all the old tests, and amazingly they all passed. There were initially many failures, but this was due to a typo in the benchmark function `words_to_length_from_regex`. I accidentally used `words_of_length` instead of `words_to_length` in the tuple comprehension, which resulted in most tuples being empty if the regex didn't match words of exactly that length. That has been fixed now. I also think I resolved the problem with brackets in the NFA edge labels, by simply trimming all brackets right before creating the NFA and *after* the elimination loop itself. This is a much cleaner solution than trying something complex in the loop itself, and I should've thought of it earlier. I was trying to be too clever, and 
probably overthinking the big-O performance by trying to minimise the number of passes over the edge labels. Anyway, now the task is to write some more tests for the other files, which shouldn't be too hard has most of the complex foundational logic is handled in the existing tests.

## 24/09/2025 - Log #12
I have written the tests for the NFA and DFA now (as opposed to the tests for the `Transition` objects). They work, but I have a constant feeling in the back of my mind that I am missing some obvious edge case that is crucial to test. However I think this feeling is inevitable and hopefully it means I am thinking in the right way and staying viligant. After all, if I could know for sure I hit all edge cases, then there would be no bugs anywhere, which is of course impossible. I think this is an inherent deficiency of human psychology - it is easy to think about the cases I know I've tested, but very hard to think of the ones I haven't caught. Try to not think of a pink elephant...

I had a very enlightening dicussion with somebody who worked in fincance today, that made me feel a lot better about the deficiencies of my tests. I had already heard stories of how complex banks' systems are, and how all of it is really held together by 'tape and string' despite the sleek exterior. First they had the original systems in the 70s, then they had to add a layer for ATMs, then for the internet, and so on. At each stage there is so much increasing complexity and compatibility concerns with the legacy code that it is virtually impossible for anybody to ensure that every single thing works with every other thing as it should do. As this person said, nobody knew that the internet would come along when they build the original systems, and how could they? This makes me think the term 'future-proofing' is somewhat self-contradictory - if you could really predict the future you wouldn't have to future proof, you could just write the code as it will be used. There is a joke that all of the worlds most important digital infrastructure is run from a single Excel spreadsheet, that was never intended for that purpose. It might be true that that joke has a kernel of truth to it. Anyway, I guess my point is that if the people that control all money in the world can admit they don't really know what they're doing when faced with such an intractable system, then I shouldn't worry too much about my silly little Python project. Nobody is going to wrongly default on their mortgage if a DFA accepts the wrong word.

## 10/10/2025 - Log #13
Well, the initial version of the project has been out on github for a couple weeks now, and it's been nice to take a break from autolang. I got a handful of testers to clone it, but I don't think anyone has had the time to look at it yet. It feels strange and exciting to have my code out on the web for the first time ever. The potential for greater public scrutiny is somewhat stressful, but at the end of the day nobody is going to be looking at it in great deal anyway.

Unfortunately my autolang retirement has been forced to a premature end, because I discovered a memory bug when generating words over an alphabet. I knew this was suboptimal and would have exponential explosion, but I naively assumed that either Python or the user OS would handle this safely. To my dismay, while I was testing in jupyter to see how the end-UX was, I tried to get the language of the example DFA for length 30, and my computer bluescreened. This really shouldn't have been surprising - computers are great because they do exactly what you tell them to do, even if that entails allocating several GB of memeory for a single list of strings. 

Fortunately this should be easy to fix, as since releasing v0.1.0 I have learned more about generators and iterators. The current `.L()` methods work by fetching the entire language upfront, and then sequentially running the DFA on each word. What needs to happen instead is giving the DFA an iterator object, so it only generates and stores the words one-by-one while it computes them, vastly improving the memory load. I do still want to retain the option of storing the whole language in a variable, as that could be useful later. But this will need an actual safety check from me - I can't rely on Python. Now I will have two types of alphabet for different uses - the current one and the more memory-safe generator versions. I think the best way to do this is with a wrapper that calls one or the other depending on an arg. But before I think about the rapidly complex-ifiying architecture, I need to just rewrite the alphabet functions in utils.py.

I said it was unfortunate I was pulled back into autolang, but that's not really true. This is very good, as it gives me an excuse to finally get the diagrams working. That is a whole other can of worms since I can't easily test matplotlib in the terminal, but that is next week's problem.

## 13/10/2025 - Log #14
The patch v0.1.1 is now finished and has been pushed to github. I'm pretty sure it has fixed the memory issue, and all of the tests still passed so everything is good. I was pleasantly surpised that I was able to implement the generators without really touching the downstream behaviour at all. I also took the oppertunity to add a generator mode to the `.L()` methods, as that could certainly be useful for processing languages later if they get very large, but it shouldn't be the default for users who just want to print the language. I thought about adding a runtime safety check at the automaton level as well, but this is more challenging: precomputing the language size of the whole set of words is easy, as it's just cominatorics; trying to precompute the number of accepted words is a lot harder as it will depend on the specific automaton - some can accept only a handful of words, while others accept all of them. I think the only way to do this without making some complex estimate is by straightforwardly running through the whole language, which is precisely what would be held off via the safety check. Therefore for now I will just trust that users are responsible enough to know about the long compute time if they give a big length. At least now the worst case is just hours of runtime, and not a system crash.

Now I'm finally ready to finish working on the transition diagrams. This was in progress way back at the start of the project in June(?), but I didn't get very far and paused development to work on the core logic, since visualising nice layouts will be tricky in general. This is the first time I'm using git branches, which is another exciting learning moment, but I'll have to be careful I do it right and merge correctly. git is such a powerful tool, but still quite complex to use for me. I have made a primary new branch for v0.2.0, and now plan to make another new branch for each added feature. Of course the largest such branch will be for transition diagrams, but there are other features I want to add now instead of v0.3.0. To me this seems to be continuing the trend of autolang being quite self-referential - first there was handling regex input and building transition tables, which natually invoked parsing and CFGs; now I'm rendering graphs, and I have to learn the tree-like structure of git commits. This is probably just coincidence - after all most mathematical ideas are similar to eachother if you squint hard enough - but I think its so interesting spotting these connections when doing a sort of 'meta' simulation project like this.

For the transition diagrams themselves, the overall logic will also be harder than I expected I think. Originally I was working in jupyter, which plots inline for easy testing and means you don't have to worry about the graphics in the backend. But now I'm working with the terminal, so matplotlib will not be able to plot directly here. The easiest solution I can think of is making a wrapper which defers to two different plotting modes - one that saves the plot as an image, and the other that plots inline for users who likely *will* be using jupyter. Either the specific mode will be passed directly, or I will try and auto-detect a GUI-backend to see if inline plotting is possible. I have no idea how to do this well, and I will probably have to sift through docs again. But anyway, I can leave this for now and just implement the 'save' mode so I can test the diagrams themselves.

The current plan is to use `networkx` to generate intermediate digraphs from the automata, and then pass this to `matplotlib` to actually render the figures. I have also seen `graphviz` mentioned a lot, along with other libraries, but that seems too complex to learn right now whilst I just get it working. Graph visualisation is such a deep rabbit hole and there are so many strategies, so I want to keep it simple for now, while retaining the option to swap out the rendering logic later if needs be. My understanding is that most Python graph libraries can use a `networkx` object to draw, so that can function as the core of the visualising feature, and the downstream drawing methods will take it from there. I want to prevent an explosion in layers of abstraction, but I think its important to keep everything quite modular here. Anyway, time to work on the new branch!

Update: Trying to research matplotlib backends is becoming extremely complex. It is one things to use the library to make nice scientific plots, it is another thing entirely to try and understand how it actually works, but I feel I need to do this to make sure the code works. I could make it slightly easier by just assuming users will be in jupyter, but even then it is not trivial, and I still need to ensure I can test in the terminal. I am trying to understand exactly what the `figure` and `axes` objects are but this is not made very clear. That is expected since these internals will be useless for the vast majority of matplotlib users. Perhaps I am trying to handle too much context within autolang, instead of letting the wider ecosystem handle it, but I just want it to work smoothly. What I've always found confusing, even when I was just plotting years ago, is how the `.show()` function behaves globally and is not attached to a specific figure object. I presume this is for a good reason, as there are very few cases where one would be working with multiple figures at once without plotting them, but it is a steep learning curve. I think I should just reign in what I need to achieve now, and make the plotting more robust in a later patch.

Update 2: This dragram drawing problem is becoming extremely complex, and I think I might be in over my head. There are a number of issues with even the small selection of DFAs I tested, which are not just jank, but actually make the diagrams completely unreadable in my view. The main ones are:
- Edges going boths ways between two states tend to overlap, which is confusing when there are arrows going both ways. I tried to fix this by addiing curvature with the `connectionstyle` arg, but it is still ambiguous which label corresponds to which direction. In most cases there is only a single label displayed anyway, which is useless.
- Loop-edges are just circles, and don't have an arrow. This is not a big deal, but is still confusing.
- Edge labels often display in awkward places, overlapping with edges or eachother. They actually have a habit of displaying directly over edge intersections, which is the worst possibility. They are also opaque, and since this is white on a white background it looks very jarring.
- The start state is not obvious at all. I anticipated this would be a problem so I used a different set of colours for the start compared to other states, but in a small DFA this cannot be distinguished at a glance. Also it doesn't matter if I use different colours if it's not clear - nobody is going to read through the docs and know this. I figured adding a floating arrow to the start state was too complex, but now it seems necessary.
- Similarly, accept states are ambiguous. As far as I can tell there is not an easy way to add a double-border with simple networkx tools, as is conventional. This might be ok to just indicate with colours (green for accept, gray otherwise), but having two shades of each to signal the start is overly convoluted.

Graph drawing is such an unbelievably complex problem, and even in my hubris I've realised just getting a prototype working will be a big challenge. I suspect force-directed rendering is not even an optimal solution for drawing automata, but this is what I have. The optimal layout is so dependent on the specific automata, that I don't know if there even is a satisfying way to do it in general. There are also more dependency problems - it seems networkx needs scipy to do kamada kawai layout, but this is an optional dependency so wasn't imported. And none of that is mentioning the matplotlib backend problem which is still looming over me. By far the worst issue is positioning the edge labels, as that is essential for understanding.

I am now at a point where I have no idea what is most important to work on. I will likely have to learn far more about the internals of how networkx and matplotlib do plotting, but even what I did today was quite cumbersome. I guess the most flexible solution would be for me to design my own bespoke graph drawing algorithm instead of hacking stuff together, but unfortunately I don't have 6-12+ months to dedicate purely to graph drawing. I do have a friend who had a similar struggle, and hopefully they can share some insight, but that was in a physics context and I doubt it will be easy to translate over. I expected a lot of these problems when I originally messed around with this in June, but I assumed future me could solve it easily when the time came. Now I am future me, and I am none the wiser.

## 14/10/2025 - Log #15
Good news, the transition diagrams have improved a lot, and the remaining issues should be easier to fix than I initially thought. I continued yesterday by adding methods to the automata to call the plotting funcs, even if the plotting itself was incomplete. This made testing and prototyping a lot easier as I could just tweak some settings and immediately call `.transition_diagram()` methods instead of having to rewrite all the function calls. To my pleasant surprise, implementing plotting for PDAs and TMs went very smoothly - the only real difference was formatting the edge labels differently, and amazingly these rendered correctly first try. I attribute this to my separation of concerns with modularising the diagram logic into multiple stages, hence I simply had to generate the individual digraphs, and then pass them all to the exact same matplotlib interface. I'm so glad this planning ahead paid off and made my life easier - it rarely does!

I have also fixed the biggest readability issue in the diagrams, which was weird edge label placement that caused overlaps and hid many labels. The edge curvature is controlled by the `connectionstyle` arg, and for this implementation it seems essential to curve them so that opposite-direction edges don't overlap. The issue was that while this arg was passed to the edges themselves, it was not passed to the *labels*. Now it is passed, and all of them are now rendering right on top of the edges, and are all visible! It is still not ideal that they break the arrow continuity, but that is a problem with the text boxes, which I can hopefully work on tomorrow.

There is naturally still a lot of jank, and the specific plotting will likely go through a lot more tweaking and iteration, but this is a great start for a prototype. I am also taking the step of factoring out most of the parameters to the `settings_visuals.py` file, to make testing easier and possibly to allow users to fine-tune their own diagrams. For example, I would like the node sizes to be larger to align with conventional style. There is still the problem of indicating the start state, but now that I've combed through the `networkx` docs more I think there are several options to do this. Adding a floating arrow would be best, but that requires going deeper into the matplotlib layer I think, so another good option could be emboldening or enlarging the state name. I'll do some testing later.

The TM diagrams still look quite horrible, but I think this is an inevitable problem since TMs have so many transitions, and the edge labels are verbose. I see no conceivable way of optimising the TM layouts in general (without becoming a `graphviz` wizard), as deciding how to layout the states ventures into *semantics*, i.e. what the individual states are actually 'doing' at the high-/implementation-level. Figuring out TM semantics for an individual machine is, I imagine, very hard even for humans (I certainly struggled with it, and there's a reason there aren't a lot of transition-level examples in Sipser), so just imagine trying to automate this for any possible TM the user could pass in. Perhaps the AI people could try this, if they weren't so busy stealing people's art. Anyway, I think it's fine to leave the jank here as it's a nontrivial problem in the best case. I'll get the general transition diagrams to a passable state in the next couple of days, and then probably move on to the next feature.


## 15/10/2025 - Log #15
I think now the diagrams are in a more-or-less usable state. I discovered several more parameters for changing the style of the graphs, but it is still not quite as flexible as I would like for automata graphs. I added black outlines to the edge labels, which doesn't fix the overlap problems, but it does make it substantially more readable even in the case where there are overlaps. I also figured out how to make the start state have a bold label. This makes it a lot easier to read the start state in most cases, even if it doesn't align with the convention of using an arrow to represent it. However, I am already diverging from the convention by having the accept states be green instead of double-circled. I actually think that this might be a benefit to the pproject, since having a slightly different visual representation of automata could encourage users to think about the underlying behaviour, instead of fixating on the specifics of the diagram style, which is not technically the theoretical representation anyway.

I will try and tweak a few more parameters to change the layout, but honestly this could keep going forever, and visualisation problems tend to enter the subjective realm anyway, so it's a lot harder to make authoritative decisions. This is why I have found this feature so frustrating - I much prefer working on the logical problems which are easier to define and test. The main question then is when I should merge into v0.2.0, probably I should hold off on this for a while. I might abandon this branch for the time being, work on another feature branch, and come back later with fresh eyes to really see if I want to change anything. I imagine the real revelations about transition diagrams won't come from my isolated development anyway, but instead from other people trying it out in their own environments. Trying to predict everything that could go wrong, and what would make the best educational impression, is somewhat like trying to not think about a pink elephant. I will simply always fail to catch everything, so I want to go back to the theory.

## 16/10/2025 - Log #16
Well, I just made it through a major rite of passage - my first nasty merge conflict. I decided to merge the transition diagrams directly into v0.2.0 since I was already working in both branches independently (bad idea), and I figured continuing with this would only cause me increasingly more problems if I held off the merge. It actually wasn't that bad to fix - git nicely handled the implementation of the `transition_diagram()` methods, so all I had to do was sort out the formatting of the imports at the top of the files. I hope everything should still be working, but at this point the codebase is becoming so complex that I can't really get a handle on all of it at once. If there are any problems, these will come up soon enough, and should be easy to fix since the feature branch was independent of all changes made in the parent branch, even if the same files were modified. I am stil in awe of the technology behind git, but I suppose the whole point is I'm not supposed to understand all the details.
The next step (of many) is figuring out how to write tests for the transition diagram features. This was a major reason for merging, as I'm now at the point that I need to de-flatten the test files folder, and doing that in the feature branch before merging would've caused untold issues. There are a few more minor things I also want to add in v0.2.0, such as making a cleaner API for `Config` objects instead of manually constructing configs within the automata methods, and possibly handling memory safety at the automata level for generating languages. 

I've decided to postpone loading automata from files, even though it's in the roadmap for this version, as that would entail so much more error handling and complex parsing that I just don't want to get into. I'm not even sure if its a useful feature - would users really care about initialising an automata form a csv or yaml when they can just type out the dict? I don't think it makes the process easier, and actually makes it more convoluted. I just thought it would be a fun problem to tackle, but I have plenty of other problems to deal with right now. The most important thing is doing the tests before starting on the other minor features, so I don't forget to add them. The janky diagrams will have to stay, and can be tweaked in a later patch. The loading form file feature will probably also be in a much later patch - it makes no sense to add it now over more theoretical features.

This point is not directly related to autolang's development, but all of this coincides with me trying to teach myself the C++ language, and finish learning the Chopin 4th Ballade, both of which are nontrivial tasks. I'm having a lot of problems balancing all these projects and managing my time, and have been working 12 hour days for the last week or so. I think I need to pause some stuff, but its so hard to prioritise one thing, without rushing something else to a hasty close. I really want to make progress on autolang, but there is no clear endpoint now, so I don't know how far I should go. I really want to move on to the CFG stuff, as I've been missing spending time on the theory, but I can't rush things - that will just cause me more problems in the future. These things take time, so I will focus on making this version good as far as I am capable, to prevent even more technical debt. I don't know what the conclusion to this paragraph is, I just want to keep going and hopefully not get an RSI. Anyway, what was I saying? Ok, I should write some tests. I hate them so much.


## 10/11/2025 - Log #17
I am back to working after a break to focus on other things. I did not intend to leave autolang for so long (typing out the log date just now made me realised almost a month has elapsed), but I have encountered new motivation to continue after reading a couple of papers on the connections between automata and group theory. This gave me some ideas about more features to add, but to do that I need to quickly get through the existing planned features.

I finished (most of) the tests for the transition diagram feature. This mainly consisted of ensuring the networkx digraphs serialise the correct data from the automata objects, which was a little tedious but not as slow as I expected. Testing beyong this point (i.e. rendering and displaying) is I think very impractical and so I left those tests empty. Testing this would entail testing the node and edge layouts, and since I believe networkx drawing algorithms are randomly seeded I think this is impossible. Similarly testing the actual images outputted is impossible with standard unit tests, as it would require some kind of analysis of the particular pixels of the image, which is functionally meaningless. I'm still not happy with the decision logic for determining the platform and most appropriate display method, but I think this is somewhat outside the scope of the project core, and issues can best be patched as they arise later. I don't think manually testing each virtual environment and matplotlib backend is a good use of time, since I can't predict everywhere that users will run the code. For now, I will settle with some awkward diagrams since the core logic is working. It could even be argued that if an automata is sufficiently complex that the graph drawing becomes convoluted, then the user couldn't derive meaningful information from the automata anyway. Probably the best method later is to create an initial diagram, and then add interactivity so the user can move the nodes itself after creation. This can certainly be done with matplotlib, but again I think its not a big priority right now.

I also implemented the feature to return the Turing machine tape as well as a simple True or False. This is done via a new `compute()` method, and was easier than expected, thanks I presume to my good design and refactoring from before. There is now an internal `_run()` method with returns both the acceptance and tape, and I think this is the cleanest solution. The alternative was adding another parameter to `accepts()` which I think is overly convoluted - I want the methods to have very narrow and well-defined purposes where possible for usability. This does make me wonder if its worth adding a similar internal method to the other automata, but right now I think that's reduntant. Eventually there needs to be a way to inspect the entire computation, not just the result, but once again that will be handled later as it requires some careful thought about the best way to design it.

With that, I think v0.2.0 is almost ready to deploy to main. I just need to do some housekeeping such as updating the readme and changelog, but after that it should be ready! One other small thing is potentially making the external libraries option dependencies, since they are only used for transition diagrams and no other features. I'm not sure if this is the best strategy. On the one hand, adding matplotlib is a non-trivial increase in memory for the install, and we are only using a tiny subset of its features here. But on the other hand, I imagine that the overwhelming majority of users will already have matplotlib (and networkx) if they are using autolang. It comes down to striking a balance between whether it is more annoying to have to install more libraries, or trying to plot a transition diagram only to encounter an error and having to install then. I will think about it after I have eaten something.

Update: I forgot to mention that I uncovered an interesting bug when testing the compute() method. While inspecting the tapes of `tm1`, I noticed that all the letters on the right of the '#' had become '0', instead of 'x' as is intended by the algorithm. This suggested there was something wrong with the transition table, and I likely wrote '0' instead of 'x' by accident. However, it was confusing because the language recognised (namely words of the form 'w#w') was correct, so this meant the bug didn't affect whether a given word was accepted, just how it handled the tape. I looked at the transition diagram in Sipser p173 to remind my self how it worked in more detail. Fairly quickly I was able to spot the problem, which was for state `q8`, which wrote '0' when seeing 'x' instead of leaving the 'x' unchanged. Fixing all instances of this table in the project did indeed resolve the issue, and was also very interesting in what it revealed. The reason the language was unaffected was because the typo occured *after* the symbol-matching phase, as `q8` is only responsible for scanning right to make sure there are no trailing symbols. This means that the tape to the right of '#' was actually all 'x's at some intermediate step, but that was then overwritten by `q8`. `q8` was happy to read 'x's and replace them with '0's, and this explains why the language was conserved, since by the time we reach `q8` then it is not possible to move left again (only accept or reject depending on trailing symbols), and therefore the wrong cells would never actually be seen by the machine after being written.

I thought this was such a fascinating and subtle bug, and was impressed it made it through this far. I'm glad I caught it while testing the tape outputs before it persisted further. After all, this is the entire point of the feature! Logging and inspecting more information about the automaton revealed a lack of understanding, which lead me back to the theory to understand the implementation more deeply, and this not only allowed the issue to be fixed, but also granted more insight into the theory. This is the exact use case I envision for users, and so I hope adding more features like this will better expose knowledge gaps to students, and encourage them to be more mindful of the specifics of the theory.


## 12/11/2025 - Log #18
v0.2.0 has now been released, and I am making good progress on the CFGs for v0.3.0. The foundational data structure has been established, and a grammar can successfully be inputted and stored. I also implemented a method to generate a union of grammars. This involved some tricky collision detection to ensure that the symbol sets remained disjoint, since otherwise it could be possible for a nonterminal to inherit rules it wasn't supposed to, or be conflated with a terminal. Both of these would be bad and potentially cause the CFGs language to change. There are likely still edge cases where an unintended collision can occur, and so I'll have to think some more about these. The theoretical examples in Sipser all assume the symbols are chosen such that a collision will never occur, which makes sense given it is more trying to illustrate the algorithmic proceedure rather than a tangential practical concern. I presume the majority of use cases here will also name symbols in a reaonable way (e.g. keeping terminals lowercase and nonterminals uppercase), but the fact that I have to handle arbitrary strings makes it somewhat unstable. I could enforce particular naming conventions (indeed this was my idea with banning certain chars for automata state names), but that somehow feels overly restrictive, and would entail more convoluted input handling which I don't think is a priority. It is difficult to strike a balance between usability and rigor - for instance in the extreme case I could just hash all the nonterminals to ensure no collisions but this would completely destroy the semantics - so I want to keep as much of the original naming conventions intact. What I think I will do instead is have some reasonable collision checks that don't catch everything, and just assume all grammars will be conventionally named for the time being. This may cause issues, but I think its more important to move on with implementing more features to build out the CFG class.

Now I have to turn to converting to Chomsky Normal Form, amongst other goals. Looking through the proof again to remind myself of the algorithm has made me realise this will be quite complicated to implement. As was the case with the regex-nfa-dfa conversion, a lot of ideas in the proof are quite intuitive to grasp as a human, but need to be thought about carefully when coding. One immediate cause for concern I notice is the final step, that is converting a rule 'A -> u1 u2 ... uk' into a sequence of unit rules. This involves introducing many new rules 'A -> u1 A1', 'A1 -> u2 A2', ..., and this will may very well cause some name collisions. This algorithm has a large amount of branching logic and many things to keep track of, so I think it will take a lot of time to work out, and require several helper methods. That is too much to think about tonight, but I am definitely excited to figure it out in the coming days.


## 14/11/2025 - #Log 19
The Chomsky Normal Form implementation is coming along nicely so far, and it's required me to solve a lot of really interesting non-trivial problems, which is exactly what I was excited about. The process of storing e-rules that were removed and generally handling a fairly complex data structure was a little finnicky, but it also feels quite thematically similar to the algorithms for regex-nfa-dfa from earlier. What I found most interesting and challenging was removing occurences of a nonterminal from a rule body in the correct combinatorial way. This was reasonably intuitive to do in the theoretical proof, mainly because the examples had no more than two occurences which made it fairly obvious. But when I have to handle an arbitrarily long rule with arbitrarily many occurences, it requires more consideration. I realised it was essentially an 'inverse subset' problem, althought this is not explicitly stated in Sipser. This means you take all occurences of the nonterminal as a set, and a subset of that set corresponds to which specific occurences should be removed for a given rule; in other words the subsets correspond to the new rules to add. Of course the empty set is excluded, as that corresponds to the inital rule with all occurences, which already stays in the grammar. I considered using `itertools.combinatations()` to generate the subsets, but I decided instead to try implementing it myself. I played around with recursion until I got it working, and the result was very satisfying. I am glad that I am starting to find recursive problems easier to implement as it is a very confusing idea at first. I feel that understanding recursion almost requires refusing to understand in the sense of following the whole process, and just focusing on the inductive logic of it. It still feels like magic, in that it doesn't work until suddenly it works perfectly, but you still don't really know why. Its possible that `itertools` would've been more performant, and it aligns with my minimal dependencies goal since it's a built-in module, but I had fun writing it myself so I'm keeping it. I highly doubt memory blowup or recursion depth will be an issue in practical cases, but we'll have to wait and see. I wrote a lot more narrow helper functions than in previous features, which I think is definitely a good plan for modularity. It is however hard to write helpers whilst simultaneously writing the method they are called by, since you have to balance ensuring they do the right purpose now but also try to generalise and anticipate their reuse elsewhere. 

All that being said, I haven't tested the e-rule removal process yet, so that will happen today and reveal any bugs. I think the best plan today is to write unit tests for all the methods I wrote yesterday instead of pushing forward with the rest of Chomsky. My heart says to go on and do the rest of the algorithm, but my head says to validate the existing code to prevent technical debt and subtle bugs from spiraling. There are still some issues with the CFG formatting I didn't catch yesterday. For example, it actually shouldn't be allowed for a rule body to be a generic Iterable, since the order is essential to the grammar's semantics, so it needs to change to Sequence. The outer container can still just be Iterable, since the rules themselvs are unordered.  There are quite a few subtleties in the structure I need to iron out to prevent unexpected behaviour, and no doubt more will arrive as CFG uses grow. But patching issues you didn't think of at the time is part of all the fun. So today the agenda is write tons of unit tests (they are getting less intimidating), polish the CFG data structure, and if I have time, continue with the Chomsky algorithm.

More generally, there are several other things I want to add to v0.3.0, but these are mainly maintenance, refactoring, and more small helpers. CFG is still the flagship feature, so I'm going to focus on that for the forseeable, and at this pace it shouldn't take too long to make passable. I am reminiscing about how earlier in the project I struggled a lot to write all the functions, simply because I was in 'analysis paralysis' trying to predict all the potential future uses and achieve an extensible level of generality and abstraction. In retrospect I probably should have just made everything for its current purpose to progress faster, since refactoring never takes as long as you expect if you understand the code well. I think as the project grows naturally, sometimes in the planned direction and sometimes somewhere else entirely, it is becoming easier to do this with my increased experience. And additionaly, the project is becoming sufficiently substantial and self-cohesive that it is more obvious how to plan things out anyway. 

Update: Today I wrote a large amount of tests for CFGs, which uncovered some bugs and implementation oversights which I then fixed. To my amazement, the main method for removing bad e-rules actually worked the first time without needing any modifications, and I think I've covered most of the important cases in isolation. This gives me a lot of confidence for proceeding to the rest of Chomsky tomorrow. The only thing I didn't finish were the methods for forming a union and renaming symbols, which work in tandem with each other. When I started thinking about the tests for them, I realised they would need more robust collision checks which I didn't want to get distracted by. Unlike the Chomsky algorithm which works on a single grammar and doesn't introduce new names (at least until the final step...), the union is tricky because it has to merge two possibly completely independent grammars together, and make all their names get along. If it was given that every grammar used nice predicable naming conventions (nonterminals always uppercase, terminals always lowercase, etc), then it would be much easier to do. But that is something that cannot be assumed, and certainly shouldn't be enforced as that would destroy the versatility of the feature, as well as being opaque and arbitrary. To handle all possibilities in a way that doesn't corrupt the generated language or unintentionally drop symbols, a careful consideration has to be made about the mutual interactions between the respective terminals and nonterminals of the grammar. In the case that there is a some name collision, the method for changing names has to absolutely guarantee all collisions are resolved, while still preserving the original names as far as possible. But it's late and I'm tired, so I will think about it tomorrow. I believe I've written around 1000 lines of code in the last two days. I'm so glad I got the majority of unit tests out of the way, so I can focus on the remaining problems with a clear conscience.


## 15/11/2025 - Log #20
A lot was achieved today. I implemented the unit rule removal process and tested it. This was an interesting problem to tackle, especially in seeing how it both resembled and differed from the epsilon removal process in more detail. Trying to come up with good examples for the unit tests was also fairly difficult. It forced me to not only test my knowledge of the algorithm by manually running it on each example, but also designing grammars that tested a broad range of properties without being too convoluted. I am fairly sure I've covered all the significant behaviours, but I don't think it's really possible to have 'complete' coverage for such a non-trivial structure. No doubt some issues will arise later.

I also uncovered a bug in the Chomsky normal form verifier method. This was very subtle as the test itself passed, successfully returning False, but it was the wrong rule that caused this, whilst the intended incorrect rule wasn't actually caught by the method. More specifically it was testing whether a rule body of length 3 would fail. It did not, because I had forgot to add any conditional for length greater than 2. Instead, the rule 'S -> AB' caused the fail, simply because I forgot to list 'B' as a nonterminal. 'B' was hence considered a terminal and the rule was not of a correct form as a consequence. I am left to hope that such an alignment of a mistake in the method *and* its test case is reasonably rare.

I spend the rest of the day cleaning up the cfg.py file by standardising the docstrings and naming conventions. This took far longer than expected, but I think it was certainly better to do it now, rather than later with many more functions to tweak. It would be a great idea to do the same for the rest of the files in the project, but there is only finite time and seemingly infinite priorities. I have a lot of general code-cleaning already on the schedule for v0.3.0, so I will get to it at some point.

Now remains the final part of the Chomsky algorithm, which is converting the remaining rules to the proper form. This actually consists of two processes: removing rules of length greater than 2; and removing rules of length 2 where not both symbols are nonterminals. These two parts are thematically similar and shouldn't differ much in their implementation. But it is not the implementation itself I'm worried about, rather the nefarious name collisions I've been putting off handling until now.

Unlike the other parts of the algorithm, these final parts introduce new nonterminals into the grammar, and that means names have to be chosen for them. There are many possible types of collisions which can easily occur if symbol names are not conventional, and even if they are conventional. 

To illustrate this, let Rule 1 be 'A -> u1 u2 ... un' where each ui is any symbol, and Rule 2 be 'A -> v1 v2' where (WLOG) v1 is a terminal. These rules correspond to each remaining part to implement, and the respective ways to eliminate them are: introduce new nonterminals A1 A2 ... An-2 and new rules 'A -> u1 A1', 'A1 -> u2 A2', ..., 'An-2 -> an-1 an'; add new nonterminal Bv1 and new rule 'Bv1 -> v1', and replace all instances of v1 in a rule body with Bv1. As has frquently been the case, in the theoretical proof the names are superfluous, and it's not hard to understand we can make arbitrarily many symbols just assert they are all distinct, but when we actually have to ennumerate all these practically and make the computer know what they all are, it's more complicated.

For Rule 1, a natural initial solution is to just call the new nonterminals 'A_1', 'A_2', ..., or similar. But what if 'A' has multiple rules for which we need to do this? Perhaps we could call them'A_1_1', 'A_1_2', ..., and 'A_2_1', 'A_2_2', ...? That seems to work, but then what if 'A_1' or 'A_1_1' is already a symbol in the grammar? Do we check for a collision for every possible new name, and if there is a collision, how do we decide what alternative name to choose? It seems pointless to break the pattern for just one collision, as that makes the final grammar significantly more intractable. The naming scheme has to work for arbitrarily many rules for a given 'A', *and* for arbitrarily many nonterminals 'A'. The situation is not much better for Rule 2, and there could also be collisions between names added by Rule 1 and Rule 2, as well as the initial symbols.

A lot of these issues could probably be handwaved away if a strict convention were imposed on the initial names (uppercase for terminals, lowercase for terminals, no underscores, etc), but that feels like a very wrong direction to try. Firstly, it is inherently arbitrary, and will likely confuse or frustrate users if their CFG is rejected for no good reason. Secondly, it is highly non-extensible and will almost certainly cause far more problems later if CFG transformations are ever chained together. Considering this has made me even want to lift the bans on the automata strings from before, as that felt unnecessarily restrictive even at the time. Furthermore none of this has accounted for the similar collision problems with the union() method, which I still haven't tackled. 

From this point there are a few possibly viable but flawed options:
- Ban a non-trivial family of strings to reserve them from this purpose. This feels like a terrible idea.
- Come up with some convoluted name generation algorithm with complex branching logic to ensure no collisions while reasonably preserving meaning. This is a broad option and certainly has potential, but it feels like it is somewhat introducing more problems than it solves.
- Just hash the new names so they are completely different, since collision aversion is a main goal of hashes anyway. This would be easy and ensure arbitrary grammars work, but would completely destroy the semantics and illustrative intentions. No thanks.

I think the best path is some combination of the above. The new names should definitely have the original nonterminal at the start, then some index metadata to record what type of elimination and what specific rule they were induced from. I should probably plan for the fact that many other future algorithms will introduce new symbols, so the name should also record which method added them (in this case Chomsky normal form). Then finally there must still be some uniqueness guarantee, in case some adversarial grammar designer happens want a terminal with a very specific and unintuitive format. This could be something like a hash or uuid, but definitely should not draw attention to itself.

I don't think there is a nice solution to this. To achieve full generality, some of the readability and conciseness will have to be lost. Perhaps I am overthinking this. The majority of users won't investigate grammars that are very large, and won't go out of their way to make weird symbol names. But it doesn't sit well with me that there is a possibility of it, however small, and if it does happen it would be exponentially harder to solve then than it would now with at least some planning. I will finish the rest of the algorithm with all this in mind, and try to ensure the individual parts can be swapped out as easily as possible. 